{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "from models.dynamics import DynamicsMaskGIT\n",
    "from models.lam import LatentActionModel\n",
    "from models.tokenizer import TokenizerVQVAE\n",
    "\n",
    "class Genie(nn.Module):\n",
    "    \"\"\"Genie model\"\"\"\n",
    "    # --- Tokenizer ---\n",
    "    in_dim: int\n",
    "    tokenizer_dim: int\n",
    "    latent_patch_dim: int\n",
    "    num_patch_latents: int\n",
    "    patch_size: int\n",
    "    tokenizer_num_blocks: int\n",
    "    tokenizer_num_heads: int\n",
    "    # --- LAM ---\n",
    "    lam_dim: int\n",
    "    latent_action_dim: int\n",
    "    num_latent_actions: int\n",
    "    lam_patch_size: int\n",
    "    lam_num_blocks: int\n",
    "    lam_num_heads: int\n",
    "    # --- Dynamics ---\n",
    "    dyna_dim: int\n",
    "    dyna_num_blocks: int\n",
    "    dyna_num_heads: int\n",
    "    dropout: float\n",
    "    mask_limit: float\n",
    "\n",
    "    def sample(self, batch):\n",
    "        temp = 1.0\n",
    "        generation_steps = 25\n",
    "        # Tokenize initial frame\n",
    "        token_idxs = self.tokenizer.vq_encode(batch[\"videos\"], training=False)['indices']\n",
    "        token_idxs = jnp.concatenate\n",
    "        lam_codes = self.lam.get_codebook[(batch[\"latent_actions\"],)]\n",
    "\n",
    "        # --- MASKGIT ---\n",
    "        def _maskgit_step(carry, step):\n",
    "            rng, seq, mask = carry\n",
    "\n",
    "            # --- Mask videos ---\n",
    "            vid_embed = self.patch_embed(batch[\"video_tokens\"])\n",
    "            if training:\n",
    "                rng1, rng2 = jax.random.split(batch[\"mask_rng\"])\n",
    "                mask_prob = jax.random.uniform(rng1, minval=self.mask_limit)\n",
    "                mask = jax.random.bernoulli(rng2, mask_prob, vid_embed.shape[:-1])\n",
    "                mask = mask.at[:, 0].set(False)\n",
    "                vid_embed = jnp.where(jnp.expand_dims(mask, -1), self.mask_token, vid_embed)\n",
    "            else:\n",
    "                mask = None\n",
    "\n",
    "            # --- Predict transition ---\n",
    "            act_embed = self.action_up(batch[\"latent_actions\"])\n",
    "            vid_embed += jnp.pad(act_embed, ((0, 0), (1, 0), (0, 0), (0, 0)))\n",
    "            logits = self.dynamics(vid_embed)\n",
    "            return dict(token_logits=logits, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "            return carry, None\n",
    "\n",
    "        jax.lax.scan\n",
    "\n",
    "        # Predict denoised frame\n",
    "\n",
    "        # Update tokens\n",
    "\n",
    "        vid_gen = genie.tokenizer.decode(\n",
    "            dyna_outputs[\"vid_gen\"],\n",
    "            video_hw=batch['videos'].shape[2:4],\n",
    "        )\n",
    "        return vid_gen\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.tokenizer = TokenizerVQVAE(\n",
    "            in_dim=self.in_dim,\n",
    "            model_dim=self.tokenizer_dim,\n",
    "            latent_dim=self.latent_patch_dim,\n",
    "            num_latents=self.num_patch_latents,\n",
    "            patch_size=self.patch_size,\n",
    "            num_blocks=self.tokenizer_num_blocks,\n",
    "            num_heads=self.tokenizer_num_heads,\n",
    "            dropout=0.0,\n",
    "            codebook_dropout=0.0,\n",
    "        )\n",
    "        self.lam = LatentActionModel(\n",
    "            in_dim=self.in_dim,\n",
    "            model_dim=self.lam_dim,\n",
    "            latent_dim=self.latent_patch_dim,\n",
    "            num_latents=self.num_latent_actions,\n",
    "            patch_size=self.lam_patch_size,\n",
    "            num_blocks=self.lam_num_blocks,\n",
    "            num_heads=self.lam_num_heads,\n",
    "            dropout=0.0,\n",
    "            codebook_dropout=0.0,\n",
    "        )\n",
    "        self.dynamics = DynamicsMaskGIT(\n",
    "            model_dim=self.dyna_dim,\n",
    "            num_latents=self.num_patch_latents,\n",
    "            num_blocks=self.dyna_num_blocks,\n",
    "            num_heads=self.dyna_num_heads,\n",
    "            dropout=self.dropout,\n",
    "            mask_limit=self.mask_limit,\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, Any], training: bool = True) -> Dict[str, Any]:\n",
    "        tokenizer_outputs = self.tokenizer.vq_encode(batch[\"videos\"], training=False)\n",
    "        lam_outputs = self.lam.vq_encode(batch[\"videos\"], training=False)\n",
    "        outputs = dict(\n",
    "            video_tokens=jax.lax.stop_gradient(tokenizer_outputs[\"indices\"]),\n",
    "            latent_actions=jax.lax.stop_gradient(lam_outputs[\"z_q\"]),\n",
    "        )\n",
    "        outputs[\"mask_rng\"] = batch[\"mask_rng\"]\n",
    "        dyna_outputs = self.dynamics(outputs, training)\n",
    "        outputs.update(dyna_outputs)\n",
    "        mle_indices = jnp.argmax(outputs[\"token_logits\"], axis=-1)\n",
    "        outputs[\"recon\"] = self.tokenizer.decode(mle_indices, batch[\"videos\"].shape[2:4])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 09:56:37.847935: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "/home/duser/.local/lib/python3.10/site-packages/orbax/checkpoint/type_handlers.py:1442: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "import time\n",
    "\n",
    "import einops\n",
    "from flax.training import orbax_utils\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "import orbax\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import wandb\n",
    "import tyro\n",
    "\n",
    "from data.dataloader import get_dataloader\n",
    "\n",
    "ts = int(time.time())\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    # Experiment\n",
    "    num_steps: int = 200_000\n",
    "    seed: int = 0\n",
    "    seq_len: int = 16\n",
    "    image_channels: int = 3\n",
    "    image_resolution: int = 64\n",
    "    file_path: str = \"/home/duser/jafar/data/coinrun.npy\"\n",
    "    # Optimization\n",
    "    batch_size: int = 36\n",
    "    min_lr: float = 3e-6\n",
    "    max_lr: float = 3e-5\n",
    "    warmup_steps: int = 5000\n",
    "    # Tokenizer\n",
    "    tokenizer_dim: int = 512\n",
    "    latent_patch_dim: int = 32\n",
    "    num_patch_latents: int = 1024\n",
    "    patch_size: int = 4\n",
    "    tokenizer_num_blocks: int = 8\n",
    "    tokenizer_num_heads: int = 8\n",
    "    tokenizer_checkpoint: str = \"/home/duser/jafar/checkpoints/tokenizer_1721468116_50000\"\n",
    "    # LAM\n",
    "    lam_dim: int = 512\n",
    "    latent_action_dim: int = 32\n",
    "    num_latent_actions: int = 6\n",
    "    lam_patch_size: int = 16\n",
    "    lam_num_blocks: int = 8\n",
    "    lam_num_heads: int = 8\n",
    "    lam_checkpoint: str = \"/home/duser/jafar/checkpoints/lam_1721469076_175000\"\n",
    "    # Dynamics\n",
    "    dyna_dim: int = 512\n",
    "    dyna_num_blocks: int = 12\n",
    "    dyna_num_heads: int = 8\n",
    "    dropout: float = 0.0\n",
    "    mask_limit: float = 0.5\n",
    "    # Logging\n",
    "    log: bool = False\n",
    "    entity: str = \"flair\"\n",
    "    project: str = \"jafari\"\n",
    "    log_interval: int = 5\n",
    "    log_image_interval: int = 250\n",
    "    ckpt_dir: str = \"/home/duser/jafar/checkpoints\"\n",
    "    log_checkpoint_interval: int = 25000\n",
    "    log_gradients: bool = False\n",
    "    # Sampling\n",
    "    checkpoint: str = \"/home/duser/jafar/checkpoints/genie_1721738387_200000\"\n",
    "\n",
    "# args = tyro.cli(Args)\n",
    "args = Args()\n",
    "rng = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "# --- Construct train state ---\n",
    "genie = Genie(\n",
    "    # Tokenizer\n",
    "    in_dim=args.image_channels,\n",
    "    tokenizer_dim=args.tokenizer_dim,\n",
    "    latent_patch_dim=args.latent_patch_dim,\n",
    "    num_patch_latents=args.num_patch_latents,\n",
    "    patch_size=args.patch_size,\n",
    "    tokenizer_num_blocks=args.tokenizer_num_blocks,\n",
    "    tokenizer_num_heads=args.tokenizer_num_heads,\n",
    "    # LAM\n",
    "    lam_dim=args.lam_dim,\n",
    "    latent_action_dim=args.latent_action_dim,\n",
    "    num_latent_actions=args.num_latent_actions,\n",
    "    lam_patch_size=args.lam_patch_size,\n",
    "    lam_num_blocks=args.lam_num_blocks,\n",
    "    lam_num_heads=args.lam_num_heads,\n",
    "    # Dynamics\n",
    "    dyna_dim=args.dyna_dim,\n",
    "    dyna_num_blocks=args.dyna_num_blocks,\n",
    "    dyna_num_heads=args.dyna_num_heads,\n",
    "    dropout=args.dropout,\n",
    "    mask_limit=args.mask_limit,\n",
    ")\n",
    "rng, _rng = jax.random.split(rng)\n",
    "image_shape = (args.image_resolution, args.image_resolution, args.image_channels)\n",
    "dummy_inputs = dict(\n",
    "    videos=jnp.zeros((args.batch_size, args.seq_len, *image_shape), dtype=jnp.float32),\n",
    "    mask_rng=_rng\n",
    ")\n",
    "rng, _rng = jax.random.split(rng)\n",
    "params = genie.init(_rng, dummy_inputs)\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "params[\"params\"].update(\n",
    "    PyTreeCheckpointer().restore(args.checkpoint)[\"model\"][\"params\"][\"params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(args.file_path, args.seq_len, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duser/jafar/data/dataloader.py:33: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.from_numpy(sequence).clone()\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    batch = jnp.array(i, dtype=jnp.float32) / 255.0\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape\n",
    "init_frame = batch[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg', img)\n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)\n",
    "\n",
    "# for i in range(generated_frames.shape[1]):\n",
    "#     imshow(np.asarray(generated_frames[0, i]*255.0))\n",
    "#     imshow(np.asarray(videos[0, i]*255.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _sample_internal(self, \n",
    "#                         batch: Dict[str, Any], \n",
    "#                         rng: jax.random.PRNGKey,\n",
    "#                         generation_steps: int = 25,\n",
    "#                         temp: float = 1.0):\n",
    "\n",
    "#     act_embed = self.action_up(batch[\"latent_actions\"])\n",
    "#     act_embed = jnp.pad(act_embed, ((0, 0), (1, 0), (0, 0), (0, 0)))\n",
    "\n",
    "#     B, T, N = batch[\"video_tokens\"].shape\n",
    "#     vid_act_embed, gen_act_embed = act_embed[:, :T], act_embed[:, T:]\n",
    "\n",
    "#     vid_embed = self.patch_embed(batch[\"video_tokens\"]) + vid_act_embed\n",
    "\n",
    "#     def gen_step(state, step):\n",
    "#         gen, mask, rng = state\n",
    "\n",
    "#         gen_embed = self.patch_embed(gen) + gen_act_embed\n",
    "#         gen_embed = jnp.where(jnp.expand_dims(mask, -1), 0, gen_embed)\n",
    "#         gen_embed = jnp.concatenate([vid_embed, gen_embed], axis=1)\n",
    "#         logits = self.dynamics(gen_embed)[:, T:]\n",
    "\n",
    "#         n_mask_toks = cosine_schedule(\n",
    "#             step, logits, N,\n",
    "#             generation_steps=generation_steps,\n",
    "#         )\n",
    "\n",
    "#         rng, rng_gen = jax.random.split(rng)\n",
    "#         next_gen = jax.random.categorical(rng_gen, logits / temp)\n",
    "\n",
    "#         p_tokens = jax.nn.softmax(logits)\n",
    "#         p_tokens = jnp.take_along_axis(p_tokens, next_gen[..., None], axis=-1).squeeze(-1) + mask\n",
    "\n",
    "#         def get_threshold(x, idx):\n",
    "#             return jax.lax.dynamic_slice(x, (idx,), (1,))[0]\n",
    "\n",
    "#         limit_indices = N - n_mask_toks\n",
    "#         p_tokens_sorted = jnp.sort(p_tokens, axis=-1)\n",
    "#         limit = jax.vmap(jax.vmap(get_threshold))(p_tokens_sorted, limit_indices)[..., None]\n",
    "#         next_mask = (p_tokens >= limit) & ~mask\n",
    "\n",
    "#         gen = jnp.where(next_mask, next_gen, gen)\n",
    "#         mask = mask | next_mask\n",
    "\n",
    "#         return (gen, mask, rng)\n",
    "\n",
    "#     mask = jnp.zeros((B, 1, N), dtype=jnp.bool)        \n",
    "#     generated_frame = jnp.zeros((B, 1 , N), dtype=jnp.int32)\n",
    "#     rng, rng_run = jax.random.split(rng)\n",
    "#     state = (generated_frame, mask, rng_run)\n",
    "\n",
    "#     for i in range(0, generation_steps):\n",
    "#         state = gen_step(state, i)\n",
    "\n",
    "#     generated_frame = state[0]\n",
    "#     vid_gen = jnp.concatenate([batch[\"video_tokens\"], generated_frame], axis=1)\n",
    "\n",
    "#     return dict(vid_gen=vid_gen)\n",
    "\n",
    "# def sample(\n",
    "#         self,\n",
    "#         params,\n",
    "#         batch,\n",
    "#         rng,\n",
    "#         generation_steps: int = 25,\n",
    "#         temp: float = 1.0,\n",
    "# ):\n",
    "#     return self.apply(params, batch, rng, generation_steps, temp, method=self._sample_internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "generation_steps = 25\n",
    "\n",
    "# Tokenize initial frame\n",
    "tokenizer_outputs = genie.tokenizer.vq_encode(batch[\"videos\"], training=False)\n",
    "lam_codes = genie.lam.get_codebook[(batch['latent_actions'],)]\n",
    "\n",
    "# --- MASKGIT ---\n",
    "\n",
    "# Construct mask frame\n",
    "genie.sample(tokenizer_outputs)\n",
    "dyna_outputs = genie.dynamics._sample_internal(\n",
    "    batch=dict(video_tokens=tokenizer_outputs[\"indices\"],\n",
    "                latent_actions=lam_codes[:, :, None, :]),\n",
    "    rng=rng,\n",
    "    generation_steps=generation_steps,\n",
    "    temp=temp,\n",
    ")\n",
    "\n",
    "vid_gen = genie.tokenizer.decode(\n",
    "    dyna_outputs[\"vid_gen\"],\n",
    "    video_hw=batch['videos'].shape[2:4],\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
